---
title: "Clustering and classificatioin "
output: html_document
---

*Boston data was loaded from MASS package.*


```{r}
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
dim(Boston)

```
*The Boston data frame has 506 row and 14 columns. Data consist of housing values in suburbs of Boston.*
*Variables are:*

*crim (per capita crime rate by town)*

*zn(proportion of residential land zoned for lots over 25,000 sq.ft)*

*indus(proportion of non-retail business acres per town)*

*chas(Charles River dummy variable (= 1 if tract bounds river; 0 otherwise))*

*nox(nitrogen oxides concentration (parts per 10 million))*

*rm(average number of rooms per dwelling)*

*age(proportion of owner-occupied units built prior to 1940)*

*dis(weighted mean of distances to five Boston employment centres)*

*rad(index of accessibility to radial highways)*

*tax(full-value property-tax rate per \$10,000*

*ptratio(pupil-teacher ratio by town*

*black(1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town*

*lstat(lower status of the population (percent)*

*medv(median value of owner-occupied homes in\$1000s)*


*More details about data can be found from link:* <https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html>.


```{r}
library(MASS)
summary(Boston)
# plot matrix of the variables
pairs(Boston)
```

*It is very hard to see the distributions of data and relationships between them, because lots of data and wide range of datapoints. Chas is binary variable, other variables has datapoits in broader distridution. Few varible sets have linear slope, and therefore they have relationship.*

*Few examples of variablees that has relationship between them:*

*medv and lstat*

*nox and dis*

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```
*Data was scaled so, the mean values is centered to 0 and distribution of variables are in range from +9.9 to - 3.9.*

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# boston_scaled is available

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

*Crime categories were predicted with LDA model. Model was made by using training set of crime categories and other variables. Model was utilized to predict the crime categories from test data set (did not include crime categories). The model predicted crime rates with following accuracies: low rate = 10/14 = 70 %, med_low rate = 15/31 = 48%, med_high rate= 22/31 =71 %, high rate = 26/26 = 100%. Results show that the model can predict crime rate quite accurately.*

Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)
